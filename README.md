# LocalLLMRAG

ãƒ­ãƒ¼ã‚«ãƒ«LLMã¨ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆQdrantï¼‰ã‚’ä½¿ç”¨ã—ãŸRAGï¼ˆRetrieval-Augmented Generationï¼‰ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚PDFã‚„ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€ãã®å†…å®¹ã«åŸºã¥ã„ã¦è³ªå•ã«å›ç­”ã™ã‚‹Flask APIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã™ã€‚

## ğŸ“‹ ç›®æ¬¡

- [æ©Ÿèƒ½](#æ©Ÿèƒ½)
- [ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆ](#ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆ)
- [ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—](#ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—)
- [ä½¿ã„æ–¹](#ä½¿ã„æ–¹)
- [APIä»•æ§˜](#apiä»•æ§˜)
- [è¨­å®šã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º](#è¨­å®šã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º)

## ğŸš€ æ©Ÿèƒ½

- **æ–‡æ›¸ã®åŸ‹ã‚è¾¼ã¿**: PDFã‚„ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦Qdrantã«ä¿å­˜
- **è³ªå•å¿œç­”**: ä¿å­˜ã•ã‚ŒãŸæ–‡æ›¸ã‹ã‚‰é–¢é€£æƒ…å ±ã‚’æ¤œç´¢ã—ã€LLMãŒå›ç­”ã‚’ç”Ÿæˆ
- **è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œ**: ä¸€åº¦ã«è¤‡æ•°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯èƒ½
- **ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œ**: ã™ã¹ã¦ã®å‡¦ç†ãŒãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å®Œçµï¼ˆå¤–éƒ¨APIã¸ã®é€šä¿¡ä¸è¦ï¼‰
- **é«˜é€Ÿã‚­ãƒ£ãƒƒã‚·ãƒ¥**: ãƒ¢ãƒ‡ãƒ«ã‚’åˆå›ãƒ­ãƒ¼ãƒ‰å¾Œã«ãƒ¡ãƒ¢ãƒªã«ä¿æŒã—ã¦é«˜é€ŸåŒ–

## ğŸ—ï¸ ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆ

- **åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«**: `sentence-transformers/all-MiniLM-L6-v2`ï¼ˆ384æ¬¡å…ƒï¼‰
- **LLM**: `Qwen2.5-0.5B-Instruct`ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ï¼‰
- **ãƒ™ã‚¯ãƒˆãƒ«DB**: Qdrantï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ï¼‰
- **Webãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**: Flask
- **ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²**: ãƒˆãƒ¼ã‚¯ãƒ³ãƒ™ãƒ¼ã‚¹ã®è´ªæ¬²åˆ†å‰²ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ä»˜ãï¼‰

## ğŸ“¦ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### 1. å¿…è¦ãªç’°å¢ƒ

- Python 3.10ä»¥ä¸Š
- ååˆ†ãªãƒ¡ãƒ¢ãƒªï¼ˆLLMãƒ­ãƒ¼ãƒ‰ã«æœ€ä½4GBæ¨å¥¨ï¼‰
- Qdrantã‚µãƒ¼ãƒãƒ¼

### 2. ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# ä»®æƒ³ç’°å¢ƒã®ä½œæˆï¼ˆæ¨å¥¨ï¼‰
python3 -m venv venv
source venv/bin/activate  # Windowsã®å ´åˆ: venv\Scripts\activate

# ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt
```

### 3. Qdrantã®èµ·å‹•

Dockerã‚’ä½¿ç”¨ã™ã‚‹å ´åˆï¼š

```bash
docker run -p 6333:6333 -p 6334:6334 \
    -v $(pwd)/qdrant_storage:/qdrant/storage \
    qdrant/qdrant
```

ã¾ãŸã¯ã€Qdrantã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦èµ·å‹•ã—ã¦ãã ã•ã„ã€‚

### 4. LLMãƒ¢ãƒ‡ãƒ«ã®æº–å‚™

ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã«`Qwen/`ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå¿…è¦ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã¯ã€HuggingFaceã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼š

```bash
# HuggingFace CLIã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆä¾‹ï¼‰
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct --local-dir Qwen/Qwen2.5-0.5B-Instruct
```

ã¾ãŸã¯`config.py`ã§åˆ¥ã®ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®šã§ãã¾ã™ã€‚

## ğŸ’¡ ä½¿ã„æ–¹

### 1. Flaskã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•

```bash
python app.py
```

ã‚µãƒ¼ãƒãƒ¼ã¯`http://localhost:1234`ã§èµ·å‹•ã—ã¾ã™ã€‚

### 2. æ–‡æ›¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆåŸ‹ã‚è¾¼ã¿ï¼‰

PDFã‚„ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦Qdrantã«ä¿å­˜ã—ã¾ã™ã€‚

**curlã®ä¾‹:**

```bash
curl -X POST http://localhost:1234/embedd \
  -F "files=@docs/sample.pdf" \
  -F "files=@docs/document.txt"
```

**Pythonã®ä¾‹:**

```python
import requests

url = "http://localhost:1234/embedd"
files = [
    ('files', open('docs/sample.pdf', 'rb')),
    ('files', open('docs/document.txt', 'rb'))
]

response = requests.post(url, files=files)
print(response.json())
```

**ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹:**

```json
{
  "success": true,
  "message": "ãƒ•ã‚¡ã‚¤ãƒ«ã®åŸ‹ã‚è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸ",
  "processed_files": 2,
  "file_names": ["sample.pdf", "document.txt"],
  "total_chunks": 45
}
```

### 3. è³ªå•ã®é€ä¿¡

ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸæ–‡æ›¸ã«åŸºã¥ã„ã¦è³ªå•ã«å›ç­”ã—ã¾ã™ã€‚

**curlã®ä¾‹:**

```bash
curl -X POST http://localhost:1234/question \
  -H "Content-Type: application/json" \
  -d '{
    "question": "ã“ã®æ–‡æ›¸ã®ä¸»ãªãƒ†ãƒ¼ãƒã¯ä½•ã§ã™ã‹ï¼Ÿ",
    "top_k": 5
  }'
```

**Pythonã®ä¾‹:**

```python
import requests

url = "http://localhost:1234/question"
data = {
    "question": "ã“ã®æ–‡æ›¸ã®ä¸»ãªãƒ†ãƒ¼ãƒã¯ä½•ã§ã™ã‹ï¼Ÿ",
    "top_k": 5
}

response = requests.post(url, json=data)
result = response.json()

print(f"è³ªå•: {result['question']}")
print(f"å›ç­”: {result['answer']}")
print(f"å‚ç…§æ–‡æ›¸æ•°: {result['num_contexts']}")
```

**ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹:**

```json
{
  "success": true,
  "question": "ã“ã®æ–‡æ›¸ã®ä¸»ãªãƒ†ãƒ¼ãƒã¯ä½•ã§ã™ã‹ï¼Ÿ",
  "answer": "ã“ã®æ–‡æ›¸ã®ä¸»ãªãƒ†ãƒ¼ãƒã¯ã€ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚’æ´»ç”¨ã—ãŸRAGã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ã§ã™ã€‚\nä¸»ãªãƒã‚¤ãƒ³ãƒˆã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š\n- ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã«ã‚ˆã‚‹é–¢é€£æƒ…å ±ã®å–å¾—\n- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”¨ã„ãŸæ­£ç¢ºãªå›ç­”ç”Ÿæˆ\n- ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã®å®Œçµã—ãŸå‡¦ç†\n\nå‚ç…§: [1],[2],[3]",
  "num_contexts": 5,
  "contexts": [
    {
      "index": 1,
      "source": "sample.pdf",
      "title": "",
      "page": "",
      "chunk_id": 0,
      "text_preview": "RAGã‚·ã‚¹ãƒ†ãƒ ã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã¨æƒ…å ±æ¤œç´¢ã‚’çµ„ã¿åˆã‚ã›ãŸæŠ€è¡“ã§ã™..."
    }
  ]
}
```

### 4. ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯

ã‚µãƒ¼ãƒãƒ¼ã®ç¨¼åƒçŠ¶æ…‹ã‚’ç¢ºèªã—ã¾ã™ã€‚

```bash
curl http://localhost:1234/health
```

**ãƒ¬ã‚¹ãƒãƒ³ã‚¹:**

```json
{
  "status": "ok",
  "message": "Flask RAG API is running"
}
```

## ğŸ“– APIä»•æ§˜

### `POST /embedd`

æ–‡æ›¸ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¾ã™ã€‚

**ãƒªã‚¯ã‚¨ã‚¹ãƒˆ:**
- Content-Type: `multipart/form-data`
- Body: `files` ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«1ã¤ä»¥ä¸Šã®ãƒ•ã‚¡ã‚¤ãƒ«

**å¯¾å¿œãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼:**
- `.txt`
- `.md`
- `.pdf`
- `.json`

**ãƒ¬ã‚¹ãƒãƒ³ã‚¹:**
```json
{
  "success": boolean,
  "message": string,
  "processed_files": number,
  "file_names": string[],
  "total_chunks": number
}
```

### `POST /question`

è³ªå•ã‚’é€ä¿¡ã—ã¦å›ç­”ã‚’å–å¾—ã—ã¾ã™ã€‚

**ãƒªã‚¯ã‚¨ã‚¹ãƒˆ:**
- Content-Type: `application/json`
- Body:
  ```json
  {
    "question": "è³ªå•æ–‡ï¼ˆå¿…é ˆï¼‰",
    "top_k": 5,  // å–å¾—ã™ã‚‹é–¢é€£æ–‡æ›¸æ•°ï¼ˆä»»æ„ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ5ï¼‰
    "source_filter": "sample.pdf"  // ç‰¹å®šãƒ•ã‚¡ã‚¤ãƒ«ã«é™å®šï¼ˆä»»æ„ï¼‰
  }
  ```

**ãƒ¬ã‚¹ãƒãƒ³ã‚¹:**
```json
{
  "success": boolean,
  "question": string,
  "answer": string,
  "num_contexts": number,
  "contexts": [
    {
      "index": number,
      "source": string,
      "title": string,
      "page": string,
      "chunk_id": number,
      "text_preview": string
    }
  ]
}
```

### `GET /health`

ã‚µãƒ¼ãƒãƒ¼ã®ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚’è¡Œã„ã¾ã™ã€‚

**ãƒ¬ã‚¹ãƒãƒ³ã‚¹:**
```json
{
  "status": "ok",
  "message": "Flask RAG API is running"
}
```

## âš™ï¸ è¨­å®šã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

`config.py`ã§å„ç¨®è¨­å®šã‚’å¤‰æ›´ã§ãã¾ã™ã€‚

### åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®è¨­å®š

```python
@dataclass
class EmbeddingCfg:
    model_name: str = "sentence-transformers/all-MiniLM-L6-v2"
    batch_size: int = 64
    normalize: bool = True
```

### Qdrantã®è¨­å®š

```python
@dataclass
class QdrantCfg:
    host: str = "127.0.0.1"
    port: int = 6333
    collection: str = "rag_docs"
```

### LLMã®è¨­å®š

#### ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ

```python
@dataclass
class LLMCfg:
    model_type: str = "local"  # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
    model_path: str = "Qwen/Qwen2.5-0.5B-Instruct"  # ãƒ­ãƒ¼ã‚«ãƒ«LLMãƒ‘ã‚¹
    # model_path: str = "Qwen/Qwen2.5-7B-Instruct"  # ã‚ˆã‚Šé«˜æ€§èƒ½ãªãƒ¢ãƒ‡ãƒ«
    dtype: str = "auto"  # "auto" / "bfloat16" / "float16"
    max_new_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
```

#### OpenAI APIã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ

```python
@dataclass
class LLMCfg:
    model_type: str = "openai"  # OpenAI APIã‚’ä½¿ç”¨
    openai_model: str = "gpt-4o-mini"  # "gpt-4o-mini" / "gpt-4o" / "gpt-3.5-turbo"
    openai_api_key: str = ""  # ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ã‹ã‚‰å–å¾—
    openai_base_url: str = ""  # ã‚«ã‚¹ã‚¿ãƒ ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆç”¨ï¼ˆç©ºã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
    max_new_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
```

#### ç’°å¢ƒå¤‰æ•°ã®è¨­å®š

OpenAI APIã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€APIã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®šã—ã¦ãã ã•ã„ï¼š

```bash
export OPENAI_API_KEY="your-api-key-here"
```

### ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã®è¨­å®š

```python
@dataclass
class ChunkCfg:
    target_tokens: int = 400      # ãƒãƒ£ãƒ³ã‚¯ã®ç›®æ¨™ãƒˆãƒ¼ã‚¯ãƒ³æ•°
    overlap_tokens: int = 60      # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³æ•°
    min_chars: int = 150          # æœ€å°æ–‡å­—æ•°
```

## ğŸ“‚ ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
LocalLLMRAG/
â”œâ”€â”€ app.py              # Flaskã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³æœ¬ä½“
â”œâ”€â”€ config.py           # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ ingest.py           # æ–‡æ›¸ã®èª­ã¿è¾¼ã¿ã¨åŸ‹ã‚è¾¼ã¿å‡¦ç†
â”œâ”€â”€ query.py            # æ¤œç´¢ã¨å›ç­”ç”Ÿæˆå‡¦ç†
â”œâ”€â”€ utils_chunk.py      # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”œâ”€â”€ requirements.txt    # ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒªã‚¹ãƒˆ
â”œâ”€â”€ README.md           # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ docs/               # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®æ–‡æ›¸ã‚’æ ¼ç´
â”œâ”€â”€ Qwen/               # LLMãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«
â””â”€â”€ qdrant_storage/     # Qdrantã®ãƒ‡ãƒ¼ã‚¿ä¿å­˜å…ˆ
```

## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### Qdrantã«æ¥ç¶šã§ããªã„

- Qdrantã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„
- `config.py`ã®ãƒ›ã‚¹ãƒˆã¨ãƒãƒ¼ãƒˆè¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„

### ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼

- ã‚ˆã‚Šå°ã•ã„LLMãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼ˆ0.5Bç‰ˆãªã©ï¼‰
- `config.py`ã§`dtype`ã‚’`"float16"`ã«è¨­å®šã—ã¦ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›ã§ãã¾ã™

### ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«æ™‚é–“ãŒã‹ã‹ã‚‹

- åˆå›ã®ã¿æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼ˆæ•°åˆ†ç¨‹åº¦ï¼‰
- 2å›ç›®ä»¥é™ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã‚‹ãŸã‚é«˜é€Ÿã§ã™

## ğŸ“ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ä¸‹ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚

## ğŸ¤ ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³

ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚„ Issue ã®å ±å‘Šã‚’æ­“è¿ã—ã¾ã™ï¼


## SSHã§ã‚¯ãƒ©ã‚¦ãƒ‰GPUã‚’ä½¿ã†å ´åˆï¼ˆç’°å¢ƒè¨­å®šï¼‰
ã‚¯ãƒ©ã‚¦ãƒ‰ä¸Šã®GPUã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆä¾‹: Ubuntu 22.04 + NVIDIA GPUï¼‰ã«SSHæ¥ç¶šã—ã¦å®Ÿè¡Œã™ã‚‹æ‰‹é †ã§ã™ã€‚

### 0) å‰æ
- GPUå¯¾å¿œãƒ‰ãƒ©ã‚¤ãƒ/NVIDIA Container Toolkit ãªã©ã¯å„ã‚¯ãƒ©ã‚¦ãƒ‰ã®æ‰‹é †ã«å¾“ã£ã¦ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚°ãƒ«ãƒ¼ãƒ—/ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«ã§å¿…è¦ãªãƒãƒ¼ãƒˆï¼ˆä¾‹: 22/6333ï¼‰ã‚’é–‹æ”¾

### 1) æ¥ç¶š
```bash
# ä¾‹: å›ºå®šIP  ã®GPU VMã«æ¥ç¶š
ssh ubuntu@<your IP> -i ~/.ssh/<your SSH key>
```

### 2) å¿…è¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
```bash
sudo apt-get update
sudo apt-get install -y git python3-venv build-essential
```

### 3) ãƒªãƒã‚¸ãƒˆãƒªé…ç½®ã¨ä»®æƒ³ç’°å¢ƒ
```bash
# ã‚µãƒ¼ãƒã«ã‚³ãƒ¼ãƒ‰ã‚’é…ç½®ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¾‹ï¼‰
# scp -r /Users/ryusei/project/mr_seino/LocalLLMRAG ubuntu@yourCloutGCPserverIP:~/

cd <your instance>
git clone https://github.com/ryusei2790/LocalLLMRAG.git

#ssh ubuntu@IP

cd LocalLLMRAG
python3 -m venv venv
source venv/bin/activate
```

### 4) ä¾å­˜ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆGPUç‰ˆPyTorchï¼‰
`requirements.txt` ã«ã¯ `torch==2.4.0` ãŒå«ã¾ã‚Œã¾ã™ã€‚GPUã‚’ä½¿ã†å ´åˆã¯CUDAã«åˆã£ãŸPyTorchãƒ“ãƒ«ãƒ‰ã‚’ä¸Šæ›¸ãã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚

```bash
# ã¾ãšé€šå¸¸ã®ä¾å­˜ã‚’å…¥ã‚Œã‚‹
pip install -U pip
pip install -r requirements.txt

pip install qdrant_client
pip install sentence_transformers
pip install pypdf
pip install accelerate
pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121





# CUDA 12.1 ç’°å¢ƒã®ä¾‹ï¼ˆç’°å¢ƒã«åˆã‚ã›ã¦å¤‰æ›´ï¼‰
# å…¬å¼ãƒ›ã‚¤ãƒ¼ãƒ«: https://pytorch.org/get-started/locally/
# pip install --force-reinstall --index-url https://download.pytorch.org/whl/cu121 torch==2.4.0 torchvision torchaudio

# å‹•ä½œç¢ºèª
python - <<'PY'
import torch
print('torch', torch.__version__, 'cuda:', torch.cuda.is_available())
print('device_count:', torch.cuda.device_count())
PY
```

- `cuda: True` ã‹ã¤ GPU ãŒ1ã¤ä»¥ä¸Šèªè­˜ã•ã‚Œã‚Œã°OK
- è¤‡æ•°GPUã®ã†ã¡ä½¿ç”¨ã™ã‚‹ã‚‚ã®ã‚’é™å®šã—ãŸã„å ´åˆã¯ `CUDA_VISIBLE_DEVICES=0` ãªã©ã‚’è¨­å®š

### 5) ãƒ¢ãƒ‡ãƒ«/ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æ°¸ç¶šåŒ–ï¼ˆä»»æ„ï¼‰
ã‚µãƒ¼ãƒå†ä½œæˆæ™‚ã®å†ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é¿ã‘ã‚‹ãŸã‚ã€Hugging Faceã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ°¸ç¶šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«è¨­å®šå¯èƒ½ã§ã™ã€‚
```bash
mkdir -p ~/hf-cache
export HF_HOME=~/hf-cache
export HF_HUB_CACHE=~/hf-cache
export TRANSFORMERS_CACHE=~/hf-cache
# å¿…è¦ã«å¿œã˜ .bashrc ã«è¿½è¨˜
```

### 6) Qdrant ã®é…ç½®æ–¹é‡
- æ¨å¥¨: GPUã‚µãƒ¼ãƒä¸Šã§Qdrantã‚‚åŒå±…ã•ã›ã‚‹
```bash
# ã‚µãƒ¼ãƒä¸Šã§Qdrantèµ·å‹•ï¼ˆæ°¸ç¶šãƒœãƒªãƒ¥ãƒ¼ãƒ ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«å‰²å½“ï¼‰
sudo docker run -d --name qdrant -p 6333:6333 -p 6334:6334 \
  -v $HOME/qdrant_storage:/qdrant/storage qdrant/qdrant:latest

```
- æ—¢å­˜ãƒ­ãƒ¼ã‚«ãƒ«PCã®Qdrantã‚’ä½¿ã†å ´åˆï¼ˆé€†ãƒˆãƒ³ãƒãƒ«ï¼‰:
  - ã‚µãƒ¼ãƒâ†’ãƒ­ãƒ¼ã‚«ãƒ«ã¸ 6333 ã‚’è»¢é€ã—ã¦ã€ã‚µãƒ¼ãƒå´ã‹ã‚‰ `127.0.0.1:6333` ã§ãƒ­ãƒ¼ã‚«ãƒ«Qdrantã«å±Šãã‚ˆã†ã«ã—ã¾ã™
```bash
# ãƒ­ãƒ¼ã‚«ãƒ«PCå´ã§å®Ÿè¡Œï¼ˆ203.0.113.10 ã¯ã‚µãƒ¼ãƒï¼‰

```
  - `config.py` ã® `QdrantCfg.host` ã‚’ `127.0.0.1` ã®ã¾ã¾ã§OKï¼ˆã‚µãƒ¼ãƒå´ãƒ—ãƒ­ã‚»ã‚¹è¦–ç‚¹ã§ãƒ­ãƒ¼ã‚«ãƒ«è»¢é€å…ˆã‚’å‚ç…§ï¼‰

### 7) ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæŠ•å…¥ã¨å®Ÿè¡Œ
```bash
# docs ã‚’ã‚µãƒ¼ãƒã¸ã‚³ãƒ”ãƒ¼ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«â†’ã‚µãƒ¼ãƒï¼‰
scp -r <your environments>/docs ubuntu@<ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®IPã‚¢ãƒ‰ãƒ¬ã‚¹>:~/LocalLLMRAG/

#scp -r /Users/ryusei/project/mr_seino/LocalLLMRAG/docs ubuntu@146-235-239-191:~/ryusei-LoRA-test/LocalLLMRAG


# å–ã‚Šè¾¼ã¿ï¼ˆã‚µãƒ¼ãƒå´ï¼‰
cd ~/LocalLLMRAG
source venv/bin/activate



python ingest.py

# æ¨è«–ï¼ˆã‚µãƒ¼ãƒå´ï¼‰
python query.py
```

ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹å ´åˆã¯ä»¥ä¸‹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚
```bash
#ã€€ingest.pyã‚’å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸã‚‰
pip install qdrant_client

pip install sentence_transformers

pip install pdfplumber

# queryå®Ÿè¡Œæ™‚ã«ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸã‚‰
pip install accelerate
```

### 8) ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹/ç²¾åº¦ã®èª¿æ•´
- `config.py` ã® `LLMCfg.model_path` ã‚’ã‚µãƒ¼ãƒä¸Šã®ãƒ‘ã‚¹ï¼ˆã¾ãŸã¯HFåï¼‰ã«å¤‰æ›´
- ãƒ¡ãƒ¢ãƒªã«å¿œã˜ã¦ `LLMCfg.dtype` ã‚’ `float16`/`bfloat16` ã«å¤‰æ›´
- ç”Ÿæˆé•·ãŒå¤§ãã„å ´åˆã¯ `max_new_tokens` ã‚’ä¸‹ã’ã‚‹ã¨ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
